# Titanic Survival Prediction using Gemini and Gemma

## Description
This repository contains code for predicting survival on the Titanic using Google's Gemini and Gemma models.

## Data
1. Download the `train.csv` and `test.csv` files from the Kaggle Titanic competition: [https://www.kaggle.com/competitions/titanic/data](https://www.kaggle.com/competitions/titanic/data)
2. Place the downloaded files in the same directory as the Python scripts.

## Gemini

### Environment Variables
Set the `GEMINI_API_KEY` environment variable with your Google Gemini API key.

### Usage
The `predict_gemini.py` script works by converting the Titanic survival prediction problem into a text completion task. For each passenger in the test set, the script provides the entire training dataset (`train.csv`) as context to the Gemini model ("gemini-1.0-pro"). The model then predicts whether the passenger survived or not based on this context.

To generate a `submission_gemini.csv` file with the survival predictions, run the script:
```bash
python predict_gemini.py
```

### Performance
The Gemini-based prediction (`predict_gemini.py`) achieved a Kaggle submission score of **0.78947**. This result outperforms the official Kaggle Titanic tutorial's score of 0.77511, which was achieved using a Random Forest classifier.

## Gemma

### Usage

#### 1. Fine-tuning Gemma-2 2b
The `gemma_2_fine_tune.py` script fine-tunes the `google/gemma-2-2b` model on the `train.csv` dataset using LoRA. The fine-tuning approach is based on a notebook published on Hugging Face: [https://www.kaggle.com/code/heidichoco/gemma-fine-tuning-for-beginners-with-huggingface](https://www.kaggle.com/code/heidichoco/gemma-fine-tuning-for-beginners-with-huggingface).

**Prerequisites:**
*   Ensure you have the necessary libraries installed (`transformers`, `trl`, `peft`, `datasets`, `torch`, `bitsandbytes`, `accelerate`).
*   Ensure `train.csv` is in the same directory.
*   A CUDA-enabled GPU is recommended for faster training.

Run the fine-tuning script:
```bash
python gemma_2_fine_tune.py
```
This will save the fine-tuned model adapter weights to the `outputs_gemma2_base` directory.

**Note:** To use the instruction-tuned version of Gemma-2 2b (`gemma-2-2b-it`) instead of the base model, modify the `model_id` variable in `gemma_2_fine_tune.py` before running the script:
```python
# Change this line in gemma_2_fine_tune.py
model_id="google/gemma-2-2b-it" 
```
Remember to adjust the `output_dir` in `load_gemma_fine_tuned.py` accordingly if you change the output directory name in `gemma_2_fine_tune.py` when using the instruction-tuned model.

#### 2. Generating Predictions with Fine-tuned Gemma
The `load_gemma_fine_tuned.py` script loads the fine-tuned Gemma model adapter from a specified checkpoint directory and uses it to predict survival for passengers in the `test.csv` file.

**Prerequisites:**
*   Ensure `test.csv` is in the same directory.
*   Ensure the fine-tuned model checkpoint exists (e.g., in `outputs_gemma2_base/checkpoint-128` after running the fine-tuning script). **Note:** You might need to adjust the `output_dir` variable within `load_gemma_fine_tuned.py` to point to the correct checkpoint directory generated by the fine-tuning script (e.g., `outputs_gemma2_base/checkpoint-128`).

Run the prediction script:
```bash
python load_gemma_fine_tuned.py
```
This will generate a `submission_gemma_base_128.csv` file with the predictions.

### Performance
*   The fine-tuned Gemma-2 2b **base** model (`load_gemma_fine_tuned.py` using `google/gemma-2-2b` fine-tuned for 128 steps) achieved a Kaggle submission score of **0.77751**.
*   The fine-tuned Gemma-2 2b **instruction-tuned** model (`load_gemma_fine_tuned.py` using `google/gemma-2-2b-it` fine-tuned for 128 steps) achieved a Kaggle submission score of **0.78468**.
